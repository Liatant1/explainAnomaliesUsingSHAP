{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import shap\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger('shap')\n",
    "logger.disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, nb_epoch=1000, batch_size=64):\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "    encoder = Dense(int(input_dim/2), activation=\"relu\", activity_regularizer=regularizers.l1(10e-7))(input_layer)\n",
    "\n",
    "    encoder = Dense(int(input_dim/4), activation=\"relu\", kernel_regularizer=regularizers.l2(10e-7))(encoder)\n",
    "\n",
    "    decoder = Dense(int(input_dim/2), activation='relu', kernel_regularizer=regularizers.l2(10e-7))(encoder)\n",
    "    \n",
    "    decoder = Dense(input_dim, activation='sigmoid', kernel_regularizer=regularizers.l2(10e-7))(decoder)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    autoencoder.summary()\n",
    "\n",
    "    # Configures the learning process of the network\n",
    "    autoencoder.compile(optimizer='adam',loss='mean_squared_error',metrics=['mse'])\n",
    "\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    # Train the autoencoder based on the best epoch, returns history object\n",
    "    autoencoder.fit(X_train, X_train, epochs=nb_epoch, batch_size=batch_size, shuffle=True, \n",
    "                    validation_split=0.1, verbose=2, callbacks=[earlystopper])\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find anomalies and return them sorted by mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_anomaly_to_explain(autoencoder, X, n_anomalies_to_explain):\n",
    "    predictions = autoencoder.predict(X)\n",
    "    square_errors = np.power(X - predictions, 2)\n",
    "    mse_series = pd.Series(np.mean(square_errors, axis=1))\n",
    "    \n",
    "    most_anomal_trx = mse_series.sort_values(ascending=False)\n",
    "    columns=[\"id\", \"mse_all_columns\"]\n",
    "    columns.extend([\"squared_error_\" + x  for x in list(X.columns)])\n",
    "    items = []\n",
    "    for x in most_anomal_trx.iteritems():\n",
    "        item = [x[0], x[1]]\n",
    "        item.extend(square_errors.loc[x[0]])\n",
    "        items.append(item)\n",
    "\n",
    "    df_anomalies = pd.DataFrame(items, columns=columns)\n",
    "    df_anomalies.set_index('id', inplace=True)\n",
    "    \n",
    "    top_anomalies_to_explain = df_anomalies.head(n_anomalies_to_explain).index\n",
    "    return top_anomalies_to_explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_features(total_squared_error, df_with_errors): # return features that cause 50% of the sum of mse\n",
    "    error = 0\n",
    "    for num_of_features, index in enumerate(df_with_errors.index):\n",
    "        error += df_with_errors.loc[index, 'err']\n",
    "        if error >= 0.5 * total_squared_error:\n",
    "            break\n",
    "    return num_of_features + 1\n",
    "\n",
    "def get_all_user_trx(X, num_of_trx):\n",
    "    return X.head(num_of_trx)\n",
    "\n",
    "def get_err_per_trx(current_trx):\n",
    "    prediction = autoencoder.predict(np.array([[current_trx]])[0])[0]\n",
    "    square_errors = np.power(current_trx - prediction, 2)\n",
    "    trx_err_in_row = pd.DataFrame({'col_name': square_errors.index, 'err': square_errors}).reset_index(drop=True)\n",
    "    total_mse = np.mean(square_errors)\n",
    "    trx_err_in_row.sort_values(by='err', ascending=False, inplace=True)\n",
    "    return trx_err_in_row, total_mse\n",
    "\n",
    "def get_highest_values(data_set): # return features that higher than mean shap\\lime value\n",
    "    n_largest_data_frame = pd.DataFrame()\n",
    "    n_explaining_features = {}\n",
    "    for i in range(len(data_set)):\n",
    "        values = data_set.iloc[i]\n",
    "        mean_val = np.mean(values)\n",
    "        n_features = 0\n",
    "        for j in range(len(values)):\n",
    "            if values[j] > mean_val:\n",
    "                n_features += 1\n",
    "        n_explaining_features[i] = n_features\n",
    "        curr_n_largest = data_set[i:i + 1].stack().nlargest(n_features)\n",
    "        n_largest_data_frame = pd.concat([n_largest_data_frame, curr_n_largest], axis=0)\n",
    "    return n_largest_data_frame, n_explaining_features\n",
    "\n",
    "def f_explain_features(X):\n",
    "    pred = autoencoder.predict(X)[:,counter]\n",
    "    return pred \n",
    "    \n",
    "def get_features_set(set_explaning_features):\n",
    "    features_set = []\n",
    "    for feature_name in np.concatenate(set_explaning_features):\n",
    "        if feature_name not in features_set:\n",
    "            features_set.append(feature_name)\n",
    "    return features_set\n",
    "\n",
    "def get_explaining_features(top_trx_to_explain, autoencoder, X_train, X_explain):\n",
    "    global counter\n",
    "    trx_counter = 0\n",
    "\n",
    "    all_set_explaning_features = {}\n",
    "\n",
    "    for trx_number in top_trx_to_explain:\n",
    "\n",
    "        trx_counter = trx_counter + 1\n",
    "        print('trx_counter', trx_counter)\n",
    "        print(trx_number)\n",
    "\n",
    "        current_trx = X_explain.loc[trx_number]\n",
    "        prediction_trx = autoencoder.predict(np.array([[current_trx]])[0])\n",
    "\n",
    "        df_err, total_mse = get_err_per_trx(current_trx)\n",
    "        num_of_features = relevant_features(total_mse*df_err.shape[0], df_err)\n",
    "\n",
    "        df_top_err = df_err.head(num_of_features)\n",
    "        all_set_explaning_features[trx_number] = []\n",
    "        values_all_features = [[] for num in range(num_of_features)]\n",
    "\n",
    "        backgroungd_set = get_all_user_trx(X_train, 200).values\n",
    "        for i in range(num_of_features):\n",
    "            counter = df_top_err.index[i]\n",
    "            explainer = shap.KernelExplainer(f_explain_features, backgroungd_set)\n",
    "            shap_values = explainer.shap_values(current_trx, nsamples='auto')\n",
    "            values_all_features[i] = shap_values \n",
    "\n",
    "        values_all_features = np.fabs(values_all_features)\n",
    "\n",
    "        contributing_data_frame = pd.DataFrame(data=values_all_features, columns=X_train.columns)  \n",
    "        highest_contributing, n_explaining_features = get_highest_values(contributing_data_frame)\n",
    "\n",
    "        for idx_explained_feature in range(num_of_features):\n",
    "            set_explaning_features = [x[1] for x in highest_contributing.index if x[0] == idx_explained_feature]\n",
    "            explaines_feature_index = df_top_err.index[idx_explained_feature]\n",
    "            set_explaning_features.insert(0, X_train.columns[explaines_feature_index])\n",
    "            \n",
    "            all_set_explaning_features[trx_number].append(set_explaning_features)\n",
    "\n",
    "        all_set_explaning_features[trx_number] = get_features_set(all_set_explaning_features[trx_number])\n",
    "       \n",
    "        \n",
    "    return all_set_explaning_features\n",
    "\n",
    "\n",
    "def explain_data(X_train, X_explain, autoencoder=None, n_anomalies_to_explain=100):\n",
    "    if autoencoder == None:\n",
    "        autoencoder = train_model(X_train)\n",
    "\n",
    "    top_trx_to_explain = get_top_anomaly_to_explain(autoencoder, X_explain, n_anomalies_to_explain)\n",
    "    \n",
    "    counter = 0\n",
    "    all_set_explaning_features_shap = get_explaining_features(top_trx_to_explain, autoencoder, X_train, X_explain)\n",
    "    return all_set_explaning_features_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_binary_data(n_records=1000000, n_anomalies=100, n_features=20):\n",
    "    features_names = []\n",
    "    for feature in range(1, n_features+1):\n",
    "        features_names.append('feature_' + str(feature))\n",
    "\n",
    "    data = np.random.randint(2, size=(n_records, n_features)) \n",
    "    data_df = pd.DataFrame(data=data, columns=features_names)\n",
    "    data_df['feature_1_2'] = data_df['feature_1'] & data_df['feature_2']\n",
    "    data_df['feature_3_4'] = data_df['feature_3'] | data_df['feature_4']\n",
    "    data_df['class'] = 0\n",
    "\n",
    "    idx = data_df.sample(n=n_anomalies).index\n",
    "\n",
    "    for i in idx:\n",
    "        rand_num = np.random.rand(1)[0]\n",
    "        if rand_num < 0.5:\n",
    "            val = data_df.loc[i, 'feature_1_2']\n",
    "            data_df.loc[i, 'feature_1_2'] = 1 - val\n",
    "            data_df.loc[i, 'class'] = '1_2'\n",
    "        else:\n",
    "            val = data_df.loc[i, 'feature_3_4']\n",
    "            data_df.loc[i, 'feature_3_4'] = 1 - val\n",
    "            data_df.loc[i, 'class'] = '3_4'\n",
    "\n",
    "    X = data_df.iloc[:,:-1]\n",
    "    y = data_df.iloc[:, -1]\n",
    "\n",
    "    train_idx = y[y==0].index.values\n",
    "    test_idx = y[y!=0].index.values\n",
    "\n",
    "    X_train = X.iloc[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get explnation of anomalies on generated random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_explain, y_explain = create_random_binary_data()\n",
    "explain_data(X_train, X_explain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
